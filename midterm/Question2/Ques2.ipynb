{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting & Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collecting data from Article search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests                                             #importing requests module for making calls to the api\n",
    "import json                                                 #importing json to write data directly to json files\n",
    "import os                                                   #importing os fetch the value of environment variable\n",
    "import time                                                 #importing time module to use sleep function\n",
    "\n",
    "for i in range (0,121):                                     #looping to get data from different pages\n",
    "    apikey_parameter={'api-key':os.environ['auth_key'],'page':i}\n",
    "                                                            #different paramenters which are to be passed as query string\n",
    "    time.sleep(4)                                           #keeping  4 secs delay between each call to the api\n",
    "    response_articlesearch=requests.get('https://api.nytimes.com/svc/search/v2/articlesearch.json',params=apikey_parameter)\n",
    "                                                            #getting responses on the get request\n",
    "    print(response_articlesearch)                           #printing the reponse to check if it is ok\n",
    "    print(response_articlesearch.url)                       #printing the url\n",
    "    \n",
    "    with open(\"data/article_search/response_articlesearch_data\"+str(i)+\".json\",\"a\") as Response_data:\n",
    "        json.dump(response_articlesearch.json(),Response_data)\n",
    "                                                            #writing data to the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collecting data from Archive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests                                             #importing requests module for making calls to the api\n",
    "import json                                                 #importing json to write data directly to json files\n",
    "import os                                                   #importing os fetch the value of environment variable\n",
    "import time                                                 #importing time module to use sleep function\n",
    "\n",
    "for i in range (0,120):                                     #looping to get data from different pages\n",
    "    apikey_parameter={'api-key':os.environ['auth_key'],'page':i}\n",
    "                                                            #different paramenters which are to be passed as query string\n",
    "    time.sleep(4)                                           #keeping  4 secs delay between each call to the api\n",
    "    \n",
    "    response_archive=requests.get('https://api.nytimes.com/svc/archive/v1/2016/1.json',params=apikey_parameter)\n",
    "                                                            #getting responses on the get request\n",
    "    print(response_archive)                                 #printing the reponse to check if it is ok\n",
    "    print(response_archive.url)                             #printing the url\n",
    "    with open(\"response_archive4.json\",\"a\") as Response_data:\n",
    "                                                            #opening a json file in append mode\n",
    "        json.dump(response_archive.json(),Response_data)\n",
    "                                                            #writing data to the json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting unique article section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~Ruchi1991/0 or inside your plot.ly account where it is named 'basic-bar'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Ruchi1991/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob                                        #importing glob library\n",
    "list_sections = [];                                          #empty list to store different categories by section_name\n",
    "import json\n",
    "\n",
    "d={}                                                         #empty dictionary to store section_name n counts\n",
    "for filename in glob('data/article_search/*.json'):          #iterrating over all the json files in data folder\n",
    "    with open(filename) as r:                                #opening each files and giving alias as r\n",
    "        json_data = json.load(r)                             #loading the data of each file into json_data\n",
    "        json_response=json_data['response']                  #getting response from each page\n",
    "        json_docs=json_response['docs']                      #getting all articles from every page\n",
    "        for doc in json_docs:                                #iterating over articles\n",
    "            if doc['section_name'] not in list_sections:  \n",
    "                list_sections.append(doc['section_name'])\n",
    "for i in range(0,len(list_sections)):\n",
    "    count=0\n",
    "    for filename in glob('data/article_search/*.json'):      #iterrating over all the json files in data folder\n",
    "        with open(filename) as r:                            #opening each files and giving alias as r\n",
    "            json_data = json.load(r)                         #loading the data of each file into json_data\n",
    "            json_response=json_data['response']\n",
    "            json_docs=json_response['docs']\n",
    "            for doc in json_docs:\n",
    "                if doc['section_name']==list_sections[i]:    #recording the count of articles for each section_name\n",
    "                    count+=1\n",
    "                    d[doc['section_name']]=count\n",
    "#print(\"Number of articles for each section\")\n",
    "#print(d)                                                     #printing each section name\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "xaxis=[];\n",
    "yaxis=[];\n",
    "for c in d.items():\n",
    "    xaxis.append(c[0])\n",
    "    yaxis.append(c[1])\n",
    "plotly.tools.set_credentials_file(username='Ruchi1991',api_key='rE53tcXa7QLlbnP8CfW9')\n",
    "data = [go.Bar(\n",
    "            x=xaxis,\n",
    "            y=yaxis\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Graph to analyze top 5/famous sections of NYT',\n",
    "    xaxis=dict(\n",
    "        title='category of articles',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=22,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number of Articles',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=22,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the maximum number of articles are on world's news.So the world's news articles can be analyzed closely to get further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analyzing what all subsections different sections have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S.\n",
      "[None, 'Politics']\n",
      "World\n",
      "['Middle East', 'Asia Pacific', 'Europe', 'Africa', 'Americas', None, 'Canada']\n",
      "Sports\n",
      "['College Basketball', None, 'Golf', 'Cricket', 'Soccer', 'Pro Basketball', 'Baseball', 'Olympics', 'Hockey', 'Pro Football', 'Tennis', 'Rugby', 'Cycling', 'Skiing', 'College Football']\n",
      "Business Day\n",
      "[None, 'DealBook', 'Retirement', 'Energy & Environment ', 'Economy', 'Media']\n",
      "Arts\n",
      "['Television', None, 'Music', 'Dance', 'Art & Design']\n",
      "T Magazine\n",
      "['Entertainment', 'Fashion & Beauty']\n",
      "Books\n",
      "['Book Review', None]\n",
      "N.Y. / Region\n",
      "[None]\n",
      "Opinion\n",
      "[None, 'Sunday Review']\n",
      "Fashion & Style\n",
      "[\"Women's Runway\", 'Weddings', None]\n",
      "NYT Now\n",
      "[None]\n",
      "Technology\n",
      "[None, 'Personal Tech']\n",
      "Travel\n",
      "[None]\n",
      "Well\n",
      "['Family', 'Live']\n",
      "Podcasts\n",
      "['The Daily']\n",
      "Briefing\n",
      "[None]\n",
      "The Upshot\n",
      "[None]\n",
      "The Learning Network\n",
      "[None]\n",
      "Real Estate\n",
      "[None]\n",
      "Your Money\n",
      "[None, \"401(k)'s and Similar Plans\"]\n",
      "Public Editor\n",
      "[None]\n",
      "Style\n",
      "[None]\n",
      "Magazine\n",
      "[None]\n",
      "Theater\n",
      "[None]\n",
      "Crosswords & Games\n",
      "[None]\n",
      "Corrections\n",
      "[None]\n",
      "Today’s Paper\n",
      "[None]\n",
      "Times Insider\n",
      "[None, 'Insider Events']\n",
      "Science\n",
      "[None]\n",
      "Movies\n",
      "[None]\n",
      "Health\n",
      "[None]\n",
      "Watching\n",
      "[None]\n",
      "Food\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "for each_sec in list_sections:\n",
    "    list_subsections=[]\n",
    "    for filename in glob('data/article_search/*.json'):          #iterrating over all the json files in data folder\n",
    "        with open(filename) as r:                                #opening each files and giving alias as r\n",
    "            json_data = json.load(r)                             #loading the data of each file into json_data\n",
    "            json_response=json_data['response']\n",
    "            json_docs=json_response['docs']\n",
    "            for doc in json_docs:\n",
    "                    if doc['section_name']==each_sec:\n",
    "                        if doc['subsection_name'] not in list_subsections:\n",
    "                            list_subsections.append(doc['subsection_name'])\n",
    "    print(each_sec)\n",
    "    print(list_subsections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analyzing subsections of world section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Ruchi1991/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_subsecs=['Middle East', 'Asia Pacific', 'Europe', 'Africa', 'Americas', None, 'Canada']\n",
    "dsubsec={}\n",
    "for i_in in range(len(list_subsecs)):\n",
    "    count=0\n",
    "    for filename in glob('data/article_search/*.json'):      #iterrating over all the json files in data folder\n",
    "        with open(filename) as r:                            #opening each files and giving alias as r\n",
    "            json_data = json.load(r)                         #loading the data of each file into json_data\n",
    "            json_response=json_data['response']\n",
    "            json_docs=json_response['docs']\n",
    "            for doc in json_docs:\n",
    "                if doc['section_name']==\"World\":         #recording the count of articles for each section_name\n",
    "                    if doc['subsection_name']==list_subsecs[i_in]:\n",
    "                        count+=1\n",
    "                        dsubsec[doc['subsection_name']]=count \n",
    "#print(\"World\")\n",
    "regions=[]\n",
    "aricles_region=[]\n",
    "for d_sub in dsubsec.items():\n",
    "#    print(d_sub)\n",
    "    regions.append(d_sub[0])\n",
    "    aricles_region.append(d_sub[1])\n",
    "\n",
    "#print(regions)\n",
    "#print(aricles_region)\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "fig = {\n",
    "    'data': [{'labels': regions,\n",
    "              'values': aricles_region,\n",
    "              'type': 'pie'}],\n",
    "    'layout': {'title': 'most talked about countries/regions in New York Times'}\n",
    "     }\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that under world section category, Europe subsection has the maximum number of articles. We can deep dive into the main headlines of Europe subcategory to know more about its articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting all the headlines for section -> World and subsection-> Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'eu': 17, 'say': 16, 'french': 11, 'talk': 10, 'leader': 8, 'fillon': 8, 'ireland': 7, 'turkey': 7, 'chief': 7, 'candidate': 7, 'syria': 7, 'german': 7, 'france': 6, 'northern': 6, 'ban': 5, 'set': 5, 'russia': 5, 'u.n.': 5, 'migrant': 5, 'end': 5, 'rally': 5, 'expert': 4, 'germany': 4, 'russian': 4, 'uk': 4, 'foreign': 4, 'british': 4, 'ahead': 4, 'poll': 4, 'trump': 4, 'presidential': 4, 'sinn': 4, 'fein': 4, 'election': 4, 'show': 4, 'syrian': 4, 'party': 3, 'state': 3, 'major': 3, 'palmyra': 3, 'u': 3, 'worker': 3, 'fire': 3, 'visit': 3, 'greek': 3, 'meeting': 3, 'berlin': 3, 'trade': 3, 'military': 3, 'european': 3, 'albania': 3, 'may': 3, 'tie': 3, 'seek': 3, 'plan': 3, 'turkish': 3, 'le': 3, 'minister': 3, 'police': 3, 'new': 3, 'found': 3, 'dutch': 3, 'right': 3, 'lawmaker': 3, 'border': 3, 'deal': 3, 'remains': 3, 'italy': 3, 'woman': 3, 'step': 2, 'islamic': 2, 'badly': 2, 'damaged': 2, 'monument': 2, 'hungarian': 2, 'ally': 2, 'stake': 2, 'medium': 2, 'merkel': 2, 'march': 2, 'contact': 2, 'airport': 2, 'travel': 2, 'opposition': 2, 'accuses': 2, 'corruption': 2, 'obama': 2, 'run': 2, 'get': 2, 'belgium': 2, 'gas': 2, 'session': 2, 'use': 2, 'agent': 2, 'going': 2, 'campaign': 2, 'next': 2, 'round': 2, 'slam': 2, 'aid': 2, 'pm': 2, 'goal': 2, 'government': 2, 'vatican': 2, 'summit': 2, 'poland': 2, 'tusk': 2, 'reform': 2, 'import': 2, 'pen': 2, 'employee': 2, 'u.s.': 2, 'boost': 2, 'agenda': 2, 'peace': 2, 'spokesman': 2, 'united': 2, 'norway': 2, 'scandal': 2, 'help': 2, 'nato': 2, 'macron': 2, 'crisis': 2, 'deepens': 2, 'edge': 2, 'voter': 2, 'head': 2, 'make': 2, 'want': 2, 'conservative': 2, 'truck': 2, 'drug': 2, 'order': 2, 'pro-erdogan': 2, 'concern': 2, 'mount': 2, 'paris': 2, 'meet': 2, 'spanish': 2, 'arrest': 2, 'serbian': 2, 'surge': 2, 'mediterranean': 2, 'far-right': 2, 'mogherini': 2, 'urge': 2, 'montenegro': 2, 'discus': 2, 'mother': 2, 'force': 2, 'principle': 2, 'call': 2, 'see': 2, 'group': 2, 'former': 2, 'finland': 1, 'eurosceptic': 1, 'soini': 1, 'helm': 1, 'recognize': 1, 'medical': 1, 'site': 1, 'inspection': 1, 'acquires': 1, 'firm': 1, 'find': 1, 'mass': 1, 'grave': 1, 'ex-catholic': 1, 'orphanage': 1, 'african': 1, 'seasonal': 1, 'dead': 1, 'italian': 1, 'washington': 1, '14': 1, 'world': 1, 'bank': 1, 'loan': 1, 'request': 1, 'push': 1, 'back': 1, 'criticism': 1, 'threaten': 1, 'strike': 1, 'fair': 1, 'premier': 1, 'centre-right': 1, 'udi': 1, 'pull': 1, 'support': 1, 'fillon-source': 1, 'considers': 1, 'intervening': 1, 'sky': 1, 'merger': 1, '21st': 1, 'century': 1, 'fox': 1, 'amnesty': 1, 'denmark': 1, 'regret': 1, 'translating': 1, 'advice': 1, 'protester': 1, 'joint': 1, 'command': 1, 'center': 1, 'mission': 1, 'campaigner': 1, 'urging': 1, 'president': 1, 'digital': 1, 'continent': 1, 'divided': 1, 'policy': 1, 'press': 1, 'integration': 1, 'release': 1, 'man': 1, 'canister': 1, 'van': 1, 'snp': 1, 'sacrificing': 1, 'scotland': 1, 'independence': 1, \"'obsession\": 1, 'infighting': 1, 'hamper': 1, 'mending': 1, 'spymaster': 1, 'movie': 1, 'ad': 1, 'diverse': 1, 'alone': 1, 'amid': 1, 'defection': 1, 'likely': 1, 'friday': 1, 'makdissi': 1, 'charity': 1, 'calais': 1, 'could': 1, 'halt': 1, 'food': 1, 'preserving': 1, 'union': 1, 'prime': 1, 'involved': 1, 'decision': 1, 'pope': 1, 'address': 1, \"'agreed\": 1, \"agenda'-diplomats\": 1, 'name': 1, 'counter-candidate': 1, 'upbeat': 1, 'tourism': 1, 'justice': 1, 'delay': 1, 'switzerland': 1, 'seal': 1, 'product': 1, 'extends': 1, 'sanction': 1, '15': 1, 'ukrainian': 1, 'loses': 1, 'ground': 1, 'comment': 1, 'kremlin': 1, 'disrupt': 1, 'balance': 1, 'power': 1, 'envoy': 1, 'progress': 1, 'pop': 1, 'star': 1, 'shakira': 1, 'carlos': 1, 'vives': 1, 'sued': 1, 'plagiarism': 1, 'trigger': 1, 'article': 1, '50': 1, 'brexit': 1, 'economic': 1, 'case': 1, 'breaking': 1, 'kingdom': 1, 'trend': 1, 'child-like': 1, 'sex': 1, 'doll': 1, 'london': 1, 'neighborhood': 1, 'evacuated': 1, 'wwii': 1, 'bomb': 1, 'lavrov': 1, \"'a\": 1, 'witch': 1, 'hunt': 1, 'ria': 1, 'awaits': 1, \"'solid\": 1, 'outline': 1, 'europe': 1, 'steer': 1, 'clear': 1, 'pin': 1, 'hope': 1, 'arab': 1, 'tourist': 1, 'greece': 1, 'bailout': 1, 'eye': 1, '2017': 1, 'bond': 1, \"'test\": 1, 'speaks': 1, 'counterpart': 1, 'bruised': 1, 'dup': 1, 'buoyant': 1, 'paradox': 1, 'far': 1, 'amidst': 1, 'rising': 1, 'prosperity': 1, 'unilateral': 1, 'offer': 1, 'post-brexit': 1, 'citizen': 1, 'wife': 1, 'battle': 1, '71': 1, 'percent': 1, 'quit': 1, 'presidency': 1, 'race': 1, 'social': 1, 'democrat': 1, 'emnid': 1, 'upends': 1, 'give': 1, 'biggest': 1, 'win': 1, 'ever': 1, 'army': 1, 'take': 1, 'village': 1, 'militant': 1, 'northwest': 1, 'official': 1, 'attacker': 1, 'autopsy': 1, 'indicates': 1, 'center-right': 1, 'mep': 1, 'replace': 1, 'council': 1, 'denies': 1, 'claim': 1, 'wiretap': 1, 'mayor': 1, 'asks': 1, 'struggling': 1, 'cancel': 1, 'plane': 1, 'believed': 1, 'crash': 1, 'near': 1, 'turkey-syria': 1, 'bring': 1, 'forward': 1, 'monday': 1, 'situation': 1, 'privatization': 1, 'agency': 1, 'disused': 1, 'project': 1, 'go': 1, 'dozen': 1, 'christian': 1, 'made': 1, 'boy': 1, '‘bleed': 1, 'jesus’': 1, 'secretary': 1, \"'tough\": 1, 'message': 1, '24': 1, 'bust': 1, 'colombian': 1, 'cocaine': 1, 'ring': 1, 'director': 1, 'lazar': 1, 'stojanovic': 1, 'dy': 1, '73': 1, 'real': 1, 'estate': 1, 'never': 1, 'materialized': 1, 'tight': 1, 'deadline': 1, 'nationalist': 1, 'figure': 1, 'deficit': 1, 'narrowed': 1, 'half': 1, '2015': 1, 'row': 1, 'bog': 1, 'accession': 1, 'ngo': 1, 'boat': 1, 'rescue': 1, '250': 1, 'creates': 1, 'landscape': 1, 'showcase': 1, 'ex-communist': 1, 'convert': 1, 'kosovo': 1, 'ratify': 1, 'difference': 1, 'respond': 1, 'writing': 1, 'question': 1, 'contacts-doj': 1, 'infant': 1, 'fetus': 1, 'ex-home': 1, 'unwed': 1, 'georgia': 1, 'suspends': 1, 'ownership': 1, 'change': 1, 'broadcaster': 1, 'euro': 1, 'court': 1, 'ruling': 1, 'resigns': 1, 'erdogan': 1, 'journalist': 1, 'held': 1, 'pkk': 1, 'member': 1, 'secret': 1, 'life': 1, 'tweet': 1, 'undoes': 1, 'celebrated': 1, 'voice': 1, 'suburb': 1, 'includes': 1, \"'terrorism\": 1, 'opposed': 1, 'planned': 1, 'referendum': 1, 'rotterdam': 1, 'booed': 1, 'parliament': 1, 'balkan': 1, 'refused': 1, 'questioned': 1, 'judge': 1, 'job': 1, 'affair': 1, 'lawyer': 1, 'hundred': 1, 'rescued': 1, 'arrival': 1, 'outpace': 1, '2016': 1, 'austria': 1, 'green': 1, 'join': 1, 'probe': 1, 'eurofighter': 1, 'accepts': 1, 'austrian': 1, 'chancellor': 1, 'eu-wide': 1, 'campaigning': 1, 'philippine': 1, 'troop': 1, 'recover': 1, 'beheaded': 1, 'hostage': 1, 'supporter': 1, \"'resist\": 1, 'turn': 1, 'paper': 1, '12': 1, 'delegate': 1, 'e.u': 1, 'visa-free': 1, 'american': 1, 'amsterdam': 1, 'artist': 1, 'hold': 1, \"'nasty\": 1, 'raise': 1, 'money': 1, 'dior': 1, 'blue': 1, 'issey': 1, 'miyake': 1, 'look': 1, 'light': 1, 'renzi': 1, 'comeback': 1, 'threatened': 1, 'public': 1, 'tender': 1, 'expects': 1, 'cannabis-growing': 1, 'program': 1, '2019': 1, 'tv': 1, 'commentator': 1, 'equality': 1, 'issue': 1, 'pro-russian': 1, 'bannon': 1, 'factbox': 1, 'rough': 1, 'guide': 1, 'manifesto': 1, 'seize': 1, 'synthetic': 1, 'worth': 1, 'million': 1, 'wall': 1, 'drone': 1, 'mine': 1, 'tightens': 1, 'incursion': 1, 'ira': 1, 'suspect': 1, '2012': 1, 'killing': 1, 'arraigned': 1, 'dublin': 1, 'queen': 1, 'elizabeth': 1, 'mark': 1, 'st': 1, 'david': 1, 'day': 1, 'leek': 1, 'gift': 1, 'polish': 1, 'intellectually': 1, 'inferior': 1, 'men': 1, 'attack': 1, 'prompt': 1, 'accord': 1, 'tunisia': 1, 'weigh': 1, 'high-level': 1, 'israel': 1, 'free': 1, 'driver': 1, 'detained': 1, 'carrying': 1, 'cylinder': 1, 'source': 1, 'separatist': 1, 'severs': 1, 'ukraine': 1, 'rail': 1, 'blockade': 1, 'kill': 1, 'two': 1, 'farm': 1, \"'ghetto\": 1, 'defy': 1, 'stop': 1, 'feeding': 1, 'ryanair': 1, 'settle': 1, 'orban': 1, 'buy': 1, 'owner': 1, 'finnish': 1, 'warns': 1, 'multi-speed': 1, 'nation': 1, 'stance': 1, 'torture': 1, 'baby': 1, 'church-run': 1, 'home': 1, 'unmarried': 1, 'damage': 1, 'ancient': 1, 'feared': 1, 'antiquity': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "counter=0\n",
    "headlines=[]\n",
    "for filename in glob('data/article_search/*.json'):      #iterrating over all the json files in data folder\n",
    "    with open(filename) as r:                            #opening each files and giving alias as r\n",
    "        json_data = json.load(r)                         #loading the data of each file into json_data\n",
    "        json_response=json_data['response']\n",
    "        json_docs=json_response['docs']\n",
    "        for doc in json_docs:\n",
    "            if doc['section_name']==\"World\":             #recording the count of articles for each section_name\n",
    "                if doc['subsection_name']==\"Europe\":\n",
    "                    headl=doc['headline']\n",
    "                    print_headline=headl['main']\n",
    "                    headlines.append(print_headline)\n",
    "                    counter+=1;\n",
    "                \n",
    "headline_text=' '.join(headlines)                        #joining all headlines to form a single text\n",
    "string_punctuation = string.punctuation\n",
    "ignoreChar=['\\r','\\n','',' ',\"'s\"]\n",
    "nums=['0','1','2','3','4','5','6','7','8','9']\n",
    "headlines_data = nltk.word_tokenize(headline_text)\n",
    "words_only = [l.lower() for l in headlines_data if l not in string_punctuation if l not in ignoreChar if l not in nums]\n",
    "filtered_headlines_data=[word for word in words_only if word not in stopwords.words('english')]\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "counts=Counter([wnl.lemmatize(data) for data in filtered_headlines_data])\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
